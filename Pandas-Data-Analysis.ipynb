{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pandas Introduction:</b>\n",
    "\n",
    "Pandas is an open-source library built in python which has capabilities to deal with large data files and perform mathematical and scientific computations to a greater extent. It is widely used by data scientist in all fields and has gained significant importance over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, the goal is to analyse sales data of an unknown store for a period of 12 months using Pandas. There are 12 files in the data directory. The files will be mergered to form a dataframe and it is used to answer few or more business questions. New columns will be added to the dataframe to make analysis simpler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires Python version of 3 and above. Please make sure Python is installed in your local computer. Additional dependencies include\n",
    "1. Pandas library installed\n",
    "2. NumPy library installed\n",
    "\n",
    "Since this is a jupyter notebook. It is assumed you have an interface which opens jupyter notebooks. For more information on how to use jupyter using Anaconda, please refer this <a href=\"https://docs.anaconda.com/anaconda/install/\">link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas, numpy, glob and os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this is taken from the following repository and cloning this reposity into local for analysis.\n",
    "\n",
    "<b>Note: The below cell needs to be run only once. Running it multiple time would create multiple copies of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the repository locally\n",
    "!git clone 'https://github.com/KeithGalli/Pandas-Data-Science-Tasks.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the analysis is inside the Sales_Data folder. It has 12 months of data starting January 2019 till December 2019. Navigating using the 'ls' command would should us the files in the particular directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls Pandas-Data-Science-Tasks/SalesAnalysis/Sales_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing multiple csv files and merging to a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to create a dataframe from multiple csv files with all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We use the library glob to create a list of all the file names. This method can be followed when files have similar names or atleast a part of the name which exists in all the files.\n",
    "2. The method glob() from the glob library takes in the argument a string with uncommon part of the file name replaced with *. * is a wild card character which means that particular part of the string is unknown or check for file names that match only the part before it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating file names\n",
    "file_names_1 = glob('Pandas-Data-Science-Tasks/SalesAnalysis/Sales_Data/Sales_*.csv')\n",
    "file_names_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use Python's os library to print the file names present in the directory as a list. This is extremely useful when we just know the directory of files but not the file names.\n",
    "- os.listdir() method takes in the argument of the folder path which has all the data files as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names using os.listdir()\n",
    "file_names_2 = os.listdir('Pandas-Data-Science-Tasks/SalesAnalysis/Sales_Data/')\n",
    "file_names_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two types of getting file names is apparant. The first method lists the entire name while the second method gives us only the file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a list of pandas dataframes from file names and concatenate to a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using file names from method 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that pandas takes in the entire file path to create a dataframe from the file. So from the first method we just need to take the file name as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using list comprehension to create a single dataframe\n",
    "list_data_frames = [pd.read_csv(file, skip_blank_lines=True) for file in file_names_1]\n",
    "\n",
    "all_data_1 = pd.concat(list_data_frames, ignore_index=True)\n",
    "\n",
    "display(all_data_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using file names from method 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the second method we just have file names without the complete directory of file. So we will concatenate the file name with the folder directory to create a complete path to pandas and we create a dataframe as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Steps done in method 2:*\n",
    "1. Created an empty dataframe to start with.\n",
    "2. Created a for loop and in each loop, a path name with the common directory and file name from method 2 is created.\n",
    "3. Loaded each csv with the path name\n",
    "4. Finally, concatenated the generated dataframe to the existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dataframe to \n",
    "all_data_2 = pd.DataFrame()\n",
    "\n",
    "for file in file_names_2:\n",
    "    path = 'Pandas-Data-Science-Tasks/SalesAnalysis/Sales_Data/' + file\n",
    "    df = pd.read_csv(path, skip_blank_lines=True)\n",
    "    all_data_2 = pd.concat([all_data_2, df], ignore_index=True)\n",
    "    \n",
    "display(all_data_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clear that, both the methods resulted in the same dataframe with all the data of 12 months.\n",
    "Now, we can go ahead to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As an initial step in the data analysis process, it is very important to make sure that the data is suitable for analysis. If it is not, we can perform relevent data wrangling steps to obtain a tidy dataframe.\n",
    " Since both the methods yielded same output, we will use any one dataframe for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking NaN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do is check in all the columns if there are any NaNs and return the rows with columns that have NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking NaN in all the columns\n",
    "all_data_1[all_data_1.isna().any(axis='columns')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we understand that there are 545 rows with NaNs.\n",
    "The approach that is intuitive here is, all the rows with NaN have all the cells filled with NaN. So it makes sense to just eliminate these rows as these don't give any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating rows with NaNs\n",
    "all_data_1 = all_data_1.dropna()\n",
    "\n",
    "# Checking again\n",
    "all_data_1[all_data_1.isna().any(axis='columns')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the rows with NaNs are elimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that there are 186305 rows but the index shows 186849 as the end point. We need to fix this. The solution is to reset index or set the index to order ID as the column itself is unique. Here, we would just do the reset index and drop the order ID column as there is no information gained from the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas function reset_index() would do the job, setting drop=True is important as it would generate a new column\n",
    "# old indexes which is not necessary and we drop the order ID column\n",
    "all_data_indexed = all_data_1.reset_index(drop=True).drop('Order ID', axis=1)\n",
    "display(all_data_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframe has proper indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While exploring to clean the dataframe a bit more, it is seen that the data has column names duplicated as rows. We need to fix this too as all rows should have identical characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "all_data_indexed['Order Date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Order Date column has cells with 'Order Date' text in it. For fixing this, let's start by converting the columns into the right data type and while doing so, this will be taken care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, what is done is\n",
    "1. Convert the Order Date column to a datetime column and coerce the errors, infer the format from the column.\n",
    "(The objective of this step to find the rows which have different datatype than and that helps in removing those rows.)\n",
    "2. After finding those rows, we get the index of those and just drop them using drop()\n",
    "3. Finally, we reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_text = all_data_indexed[pd.to_datetime(all_data_indexed['Order Date'], \n",
    "                                               errors='coerce', \n",
    "                                               infer_datetime_format=True).isna()].index\n",
    "\n",
    "all_data_indexed = all_data_indexed.drop(indices_text).reset_index(drop=True)\n",
    "all_data_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have a tidier dataframe. Now, let's add few columns for easy data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting all columns into appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Order Date'] = pd.to_datetime(all_data_indexed['Order Date'], \n",
    "                                                errors='coerce', infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Product'] = all_data_indexed['Product'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Quantity Ordered'] = all_data_indexed['Quantity Ordered'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Price Each'] = all_data_indexed['Price Each'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Purchase Address'] = all_data_indexed['Purchase Address'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Month column\n",
    "\n",
    "It can be obtained from the Order Date column. Since the year is same for all dates, no need to add year column. Month column helps in finding sales per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do is text split address via , and get second element for city and third element for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding City column\n",
    "\n",
    "It's easy to define a function and apply that function to the address column to get city and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Month'] = all_data_indexed['Order Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(address):\n",
    "    return address.split(',')[1].strip(' ')\n",
    "\n",
    "def get_state(address):\n",
    "    return address.split(',')[2].split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                 City\n0       New York City\n1       New York City\n2       New York City\n3       San Francisco\n4             Atlanta\n...               ...\n185945         Boston\n185946  New York City\n185947  San Francisco\n185948  San Francisco\n185949        Atlanta\n\n[185950 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>City</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>New York City</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>New York City</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>New York City</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>San Francisco</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>Atlanta</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>185945</td>\n      <td>Boston</td>\n    </tr>\n    <tr>\n      <td>185946</td>\n      <td>New York City</td>\n    </tr>\n    <tr>\n      <td>185947</td>\n      <td>San Francisco</td>\n    </tr>\n    <tr>\n      <td>185948</td>\n      <td>San Francisco</td>\n    </tr>\n    <tr>\n      <td>185949</td>\n      <td>Atlanta</td>\n    </tr>\n  </tbody>\n</table>\n<p>185950 rows × 1 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       State\n0         NY\n1         NY\n2         NY\n3         CA\n4         GA\n...      ...\n185945    MA\n185946    NY\n185947    CA\n185948    CA\n185949    GA\n\n[185950 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>NY</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>NY</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>NY</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>GA</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>185945</td>\n      <td>MA</td>\n    </tr>\n    <tr>\n      <td>185946</td>\n      <td>NY</td>\n    </tr>\n    <tr>\n      <td>185947</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <td>185948</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <td>185949</td>\n      <td>GA</td>\n    </tr>\n  </tbody>\n</table>\n<p>185950 rows × 1 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "all_data_indexed['City'] = all_data_indexed['Purchase Address'].apply(lambda x: get_city(x))\n",
    "all_data_indexed['State'] = all_data_indexed['Purchase Address'].apply(lambda x: get_state(x))\n",
    "\n",
    "display(all_data_indexed[['City']])\n",
    "display(all_data_indexed[['State']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Total Sales'] = all_data_indexed['Quantity Ordered']*all_data_indexed['Price Each']\n",
    "all_data_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, 'Data Analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the analysis as per the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Which month had highest revenue and how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_by_month = all_data_indexed.groupby('Month').agg(np.sum)[['Total Sales']].sort_values('Total Sales', \n",
    "                                                                                            ascending = False)\n",
    "sales_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting bar plot for revenue in each month\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(sales_by_month.index, sales_by_month['Total Sales'])\n",
    "ax.set_xticks(sales_by_month.index)\n",
    "\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Total Sales in USD')\n",
    "ax.set_title('Total Sales vs Month')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this kind of plots help in visualising more about sales, let's define a function that helps in plotting similar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(x, y, xlabel, ylabel, title):\n",
    "    \"\"\"Function to plot with given x, y, xlabel, ylabel and title\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So December being the month with highest revenue and January being the month with lower revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Which city(store) has the highest revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_by_city = all_data_indexed.groupby(['City']).agg(np.sum)[['Total Sales']].sort_values('Total Sales', \n",
    "                                                                                            ascending=False)\n",
    "sales_by_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(sales_by_city.index, sales_by_city['Total Sales'], 'City', 'Total Sales in USD', 'Total Sales vs City')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, San Francisco has the highest revenue and Austin has the lowest revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What are the best selling products in each month? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_prdt_month = all_data_indexed.groupby(['Month','Product']).agg(np.sum)[['Quantity Ordered']]\n",
    "sales_prdt_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all products sold along with the quantity in each month. Let's see in a plot. The idea is to create a subplot figure with each month products sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subplots to display quantity sold for each product in each month.\n",
    "fig, ax = plt.subplots(3,4, sharex = True, figsize=(15,15), constrained_layout=True)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        ax[i,j].bar(sales_prdt_month.loc[(i+1)*(j+1)].index, \n",
    "                    sales_prdt_month.loc[(i+1)*(j+1)]['Quantity Ordered'])\n",
    "        ax[i,j].set_xticklabels(sales_prdt_month.loc[(i+1)*(j+1)].index, rotation=90)\n",
    "\n",
    "fig.suptitle('Monthly sales of products sold', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indexed['Order Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}